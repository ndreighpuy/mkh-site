"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"intro","metadata":{"permalink":"/intro","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-03-25-welcome/index.md","source":"@site/blog/2025-03-25-welcome/index.md","title":"Intro","description":"Hello everyone -","date":"2025-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"intro","permalink":"/tags/intro"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Muhamad Khadaffy","title":"Full Stack Dev","url":"https://github.com/reighpuy","page":{"permalink":"/authors/khadaffy"},"socials":{"instagram":"https://www.instagram.com/reighpuy","github":"https://github.com/reighpuy"},"imageURL":"https://github.com/reighpuy.png","key":"khadaffy"}],"frontMatter":{"slug":"intro","title":"Intro","authors":"khadaffy","tags":["intro"]},"unlisted":false,"nextItem":{"title":"Web Scraping using Python","permalink":"/web-scraping-using-python"}},"content":"Hello everyone -\\n\\nThis is Muhamad Khadaffy, the creator of this blog.\\nWhat you are visiting right now is my personal blog, Here i express myself, What i have made, I write it here."},{"id":"web-scraping-using-python","metadata":{"permalink":"/web-scraping-using-python","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2018-11-24-web-scraping-using-python.md","source":"@site/blog/2018-11-24-web-scraping-using-python.md","title":"Web Scraping using Python","description":"Webscraping - BeautifulSoup","date":"2018-11-24T00:00:00.000Z","tags":[{"inline":true,"label":"webscrape","permalink":"/tags/webscrape"},{"inline":true,"label":"scraping","permalink":"/tags/scraping"},{"inline":true,"label":"python","permalink":"/tags/python"},{"inline":true,"label":"beautifulsoup","permalink":"/tags/beautifulsoup"}],"readingTime":2.75,"hasTruncateMarker":true,"authors":[{"name":"Muhamad Khadaffy","title":"Full Stack Dev","url":"https://github.com/reighpuy","page":{"permalink":"/authors/khadaffy"},"socials":{"instagram":"https://www.instagram.com/reighpuy","github":"https://github.com/reighpuy"},"imageURL":"https://github.com/reighpuy.png","key":"khadaffy"}],"frontMatter":{"slug":"web-scraping-using-python","title":"Web Scraping using Python","authors":"khadaffy","tags":["webscrape","scraping","python","beautifulsoup"]},"unlisted":false,"prevItem":{"title":"Intro","permalink":"/intro"},"nextItem":{"title":"Base64 Encoder in Python","permalink":"/base64-encoder-in-python"}},"content":"![Webscraping - BeautifulSoup](https://miro.medium.com/v2/resize:fit:720/format:webp/1*atk74YuoKuEn_b-Qs8nOgg.png)\\r\\n\\r\\n<center><em>Image by: Mohammad Reza</em></center><br></br>\\r\\n\\r\\nPython stands out as an excellent choice for web scraping due to its inherent simplicity and readability, making it accessible for both beginners and experienced coders. Its true power in this domain, however, lies in its rich ecosystem of specialized libraries. Notably, Beautiful Soup excels at parsing the intricate structure of HTML and XML documents, acting as your guide to easily navigate and pinpoint desired elements within a webpage. Complementing this, the Requests library provides the essential functionality to send HTTP requests, effectively fetching the content of web pages for your scraping endeavors. Furthermore, the vast and active Python community ensures ample resources, tutorials, and support are readily available should you encounter any challenges along your scraping journey.\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n### A Pretty Simple Example\\r\\n\\r\\nUsing `BeautifulSoup` -\\r\\n\\r\\n```python\\r\\nimport requests\\r\\nfrom bs4 import BeautifulSoup\\r\\n\\r\\n# The URL of the webpage we want to scrape\\r\\nurl = \\"https://www.example.com\\"\\r\\n\\r\\n# Send an HTTP GET request to the URL\\r\\nresponse = requests.get(url)\\r\\nresponse.raise_for_status() # Raise an exception for bad status codes\\r\\n\\r\\n# Parse the HTML content using Beautiful Soup\\r\\nsoup = BeautifulSoup(response.content, \'html.parser\')\\r\\n\\r\\n# Find the title tag\\r\\ntitle_tag = soup.find(\'title\')\\r\\n\\r\\n# Extract the text from the title tag\\r\\nif title_tag:\\r\\n    title = title_tag.text\\r\\n    print(f\\"The title of the page is: {title}\\")\\r\\nelse:\\r\\n    print(\\"Could not find the title tag.\\")\\r\\n```\\r\\n\\r\\n**In this code**:\\r\\n\\r\\n1. We import the `requests` and `BeautifulSoup` libraries.\\r\\n2. We define the `url` of the webpage we want to scrape.\\r\\n3. `requests.get(url)` sends a request to the specified URL and retrieves the HTML content.\\r\\n4. `response.raise_for_status()` is good practice to check if the request was successful (no error codes).\\r\\n5. `BeautifulSoup(response.content, \'html.parser\')` creates a BeautifulSoup object, parsing the HTML content. `\'html.parser\'` is the parser we\'re using.\\r\\n6. `soup.find(\'title\')` searches the parsed HTML for the first `<title>` tag.\\r\\n7. `title_tag.text` extracts the text content within the `<title>` tag.\\r\\n\\r\\n## Ethical Considerations - Scraping Responsibly\\r\\n\\r\\nIt\'s crucial to talk about ethics when it comes to web scraping. Remember that websites are someone else\'s property, and we need to be respectful. Here are a few key points to keep in mind:\\r\\n\\r\\n- Most websites have a robots.txt file that specifies which parts of the site should not be accessed by bots (including your scraper). Always respect these rules. You can usually find it at yourwebsite.com/robots.txt.\\r\\n- Make your requests at a reasonable rate to avoid overwhelming the website\'s server. Implement delays between requests.\\r\\n- Some websites explicitly prohibit scraping in their terms of service. Make sure you\'re not violating these terms.\\r\\n- Avoid extracting excessive data that you won\'t actually use.\\r\\n- Be mindful of how you use the scraped data and avoid infringing on copyrights or privacy.\\r\\n\\r\\n## What Else Can I Do?\\r\\n\\r\\nAs I become more comfortable with the basics, I can explore more advanced techniques such as:\\r\\n\\r\\n1. Scraping data from websites with pagination.\\r\\n2. Handling websites that use JavaScript to load content (you might need tools like Selenium for this).\\r\\n3. More precise ways to target specific elements in the HTML.\\r\\n4. Saving your extracted data into various formats like CSV, JSON, or databases.\\r\\n\\r\\n> So, are you ready to dive in? Start with the basics, explore the power of `requests` and `BeautifulSoup`, Happy scraping and always remember to scrape responsibly."},{"id":"base64-encoder-in-python","metadata":{"permalink":"/base64-encoder-in-python","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2018-09-07-base64-encoder-in-python.md","source":"@site/blog/2018-09-07-base64-encoder-in-python.md","title":"Base64 Encoder in Python","description":"Base64","date":"2018-09-07T00:00:00.000Z","tags":[{"inline":true,"label":"base64","permalink":"/tags/base-64"},{"inline":true,"label":"encoder","permalink":"/tags/encoder"},{"inline":true,"label":"python","permalink":"/tags/python"}],"readingTime":2.445,"hasTruncateMarker":true,"authors":[{"name":"Muhamad Khadaffy","title":"Full Stack Dev","url":"https://github.com/reighpuy","page":{"permalink":"/authors/khadaffy"},"socials":{"instagram":"https://www.instagram.com/reighpuy","github":"https://github.com/reighpuy"},"imageURL":"https://github.com/reighpuy.png","key":"khadaffy"}],"frontMatter":{"slug":"base64-encoder-in-python","title":"Base64 Encoder in Python","authors":"khadaffy","tags":["base64","encoder","python"]},"unlisted":false,"prevItem":{"title":"Web Scraping using Python","permalink":"/web-scraping-using-python"},"nextItem":{"title":"How I Learned Command Line","permalink":"/how-i-learned-command-line"}},"content":"![Base64](https://marquesfernandes.com/wp-content/uploads/2020/02/base64-wallpaper.jpg)\\r\\n\\r\\n<center><em>Image by: marquesfernandes.com</em></center><br></br>\\r\\n\\r\\nThink of Base64 as a translator. It takes binary data - the raw Os and 1s that computers understand - and converts it into a squence of printable ASCII characters. Why? because some systems are designed to handle text-based data ore reliably than raw binary. Imagine trying to send an image as a series of raw bytes through an email system that expects text! That\'s where Base64 comes to rescue.\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\nIt\'s important to understand that **Base64 is not encryption**. It\'s an encoding scheme, meaning it\'s designed to make data transmitable and readable in text-based environments, not to hide it securely. Anyone with the right tools can easily decode Base64 back to its original form.\\r\\n\\r\\n**You might encounter Base64 in various scenarios, such as:**\\r\\n\\r\\n- Encoding binary files so they can be included in email messages.\\r\\n- Embedding small files (like image or fonts) directly within HTML or CSS.\\r\\n- Encoding usernames and password for simple HTTP authentication.\\r\\n- Storing binary data within text-based configuration files like JSON or XML.\\r\\n\\r\\n## Introducing the Base64 Module\\r\\n\\r\\nPython, being the versatile language it is, provides a built-in module called `base64` that makes working with Base64 encoding and decoding a breeze. Let\'s explore how to use it.\\r\\n\\r\\n## Encoding Data with base64.b64encode()\\r\\n\\r\\nThe primary function for encoding in the base64 module is `b64encode()`. It takes a `bytes-like` object as input and returns the base64 encoded data, also as a bytes object. This is a key point: you\'ll often need to convert your strings to bytes before encoding.\\r\\n\\r\\n**Simple Code Example**:\\r\\n\\r\\n```py\\r\\nimport base64\\r\\n\\r\\noriginal_string = \\"Hello, Python Base64!\\"\\r\\nbyte_data = original_string.encode(\'utf-8\')\\r\\nbase64_encoded = base64.bs64encode(byte_data)\\r\\nprint(f\\"Original string: {original_string}\\")\\r\\nprint(f\\"Base64 encoded (bytes): {base64_encoded})\\r\\nbase64_string = base64_encoded.decode(\'utf-8\')\\r\\nprint(f\\"base64 encoded (string): {base64_string}\\")\\r\\n```\\r\\n\\r\\n**In this code:**\\r\\n\\r\\n- We import the base64 module.\\r\\n- We have our original string.\\r\\n- We use .encode(\'utf-8\') to convert the string into a sequence of bytes. UTF-8 is a common and recommended encoding for text.\\r\\n- We call `base64.b64encode()` with the byte data. The result `base64_encoded_bytes` is also in bytes format.\\r\\n- Finally, we use `.decode(\'utf-8\')` to convert the encoded bytes back into a regular string, making it easier to read and display.\\r\\n\\r\\n## Encoding Binary Files the Python Way\\r\\n\\r\\nWhat about encoding the contents of a file. like an image or a document? Here\'s how you can do it.\\r\\n\\r\\n```py\\r\\nimport base64\\r\\n\\r\\ndef encode_file_to_base64(file_path):\\r\\n    \\"\\"\\"Encodes the content of a file to a Base64 string.\\"\\"\\"\\r\\n    try:\\r\\n        with open(file_path, \'rb\') as file:\\r\\n            binary_data = file.read()\\r\\n            base64_encoded_data = base64.b64encode(binary_data).decode(\'utf-8\')\\r\\n            return base64_encoded_data\\r\\n    except FileNotFoundError:\\r\\n        return f\\"Error: File not found at {file_path}\\"\\r\\n\\r\\nimage_path = \\"your_image.jpg\\"\\r\\nencoded_image_data = encode_file_to_base64(image_path)\\r\\n\\r\\nif isinstance(encoded_image_data, str) and encoded_image_data.starswith(\\"Error\\"):\\r\\n    print(encoded_image_data)\\r\\nelse:\\r\\n    print(f\\"Base64 encoded data of {image_ath} (first 200 characters):\\\\n{encoded_image_data[:200]}...\\")\\r\\n```\\r\\n\\r\\n**In this snippet**:\\r\\n\\r\\n1. We defined a function `base64_file_to_base64` that takes the file path as input.\\r\\n2. We open the file in binary read mode(\'rb\') because we\'re dealing with raw file data.\\r\\n3. We read"},{"id":"how-i-learned-command-line","metadata":{"permalink":"/how-i-learned-command-line","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2018-06-15-how-i-learned-cl.md","source":"@site/blog/2018-06-15-how-i-learned-cl.md","title":"How I Learned Command Line","description":"Command Line","date":"2018-06-15T00:00:00.000Z","tags":[{"inline":true,"label":"command line","permalink":"/tags/command-line"},{"inline":true,"label":"cmd","permalink":"/tags/cmd"},{"inline":true,"label":"cli","permalink":"/tags/cli"}],"readingTime":3.81,"hasTruncateMarker":true,"authors":[{"name":"Muhamad Khadaffy","title":"Full Stack Dev","url":"https://github.com/reighpuy","page":{"permalink":"/authors/khadaffy"},"socials":{"instagram":"https://www.instagram.com/reighpuy","github":"https://github.com/reighpuy"},"imageURL":"https://github.com/reighpuy.png","key":"khadaffy"}],"frontMatter":{"slug":"how-i-learned-command-line","title":"How I Learned Command Line","authors":"khadaffy","tags":["command line","cmd","cli"]},"unlisted":false,"prevItem":{"title":"Base64 Encoder in Python","permalink":"/base64-encoder-in-python"}},"content":"![Command Line](https://www.dataquest.io/wp-content/uploads/2021/04/command-line-courses-dataquest-1000x520-1-1.gif)\\n\\n<center><em>Image by: dataquest.io</em></center><br></br>\\n\\nRemember that feeling of staring at a black screen with blinking text, feeling completely lost? Yeah, that was me, facing the command line for the first time. It looked intimidating, like some ancient language only understood by tech wizards. I used to think it was just for \\"hackers\\" and those who lived and breathed code. But guess what? I was wrong. So, so wrong.\\n\\nI started this journey because I was tired. Tired of feeling like my computer was dictating my workflow. I wanted more control, more efficiency. And I\'d heard whispers that the command line was the key to unlocking that. Turns out, those whispers were spot on. Learning the command line opened up a whole new world, a world where I could automate tasks, manipulate files, and more.\\n\\n\x3c!-- truncate --\x3e\\n\\n## So, let\'s begin\\n\\nLet\u2019s be honest, my initial encounters were\u2026rough. The sheer number of commands felt overwhelming. `ls`, `cd`, `mkdir`, `rm` \u2013 they were just random letters at first. And the syntax? Don\u2019t even get me started. Errors popped up like mushrooms after a rainstorm, and I felt like I was constantly breaking things.\\n\\nBut I persevered, and here\u2019s what helped me:\\n\\n- Online Tutorials and Courses \u2013 [**Codecademy**](https://www.codecademy.com/) and [**freeCodeCamp**](https://www.freecodecamp.org/) were great. They broke down complex concepts into digestible chunks.\\n- Visual explanations are very helpful \u2013 found this on YouTube. I found a channel that walked me through the commands step by step.\\n- \\"The Linux Command Line\\" by William Shotts \u2013 This book became my trusty companion, a comprehensive guide to navigating the command line.\\n- Stack Overflow \u2013 This site became my best friend.\\n- Then, I practice. Even 20 mins a day made a big difference.\\n\\n## Here\'s a breakdown of my learning strategies:\\n\\n- Start with the basics: Mastering `ls`, `cd`, `mkdir`, `rm`, and `pwd` is _crucial_.\\n- Consistency is key. Even short sessions add up.\\n- Create a cheat sheet (Keep your frequently used commands handy).\\n- Understand the concepts (Don\'t just memorize, understand why the commands work).\\n- Learn about file paths \u2013 Navigating the file system is essential.\\n\\nWhen I finally understood how pipes and redirects worked, it felt like unlocking a secret level in a video game.\\n\\n\\"**So, where does all this command-line knowledge actually take you?** For me, it\'s woven into my daily workflow. I can manage files and directories, moving, renaming, and deleting with simple commands, saving me tons of time. Automating repetitive tasks with custom scripts has been a game-changer, freeing me up to focus on more creative work. When it comes to version control with Git, the command line is my best friend, allowing for precise control. Connecting to remote servers via SSH is seamless, letting me manage projects from anywhere. And for web development, it\'s indispensable\u2014running servers, building projects, and deploying code becomes far more efficient. For example, if I need to quickly search for specific text within files, I use `grep -r \\"example\\" *` to find all instances of \'example\' in my current directory and its subdirectories. Or when I need to locate specific files, `find . -name \\"*.txt\\"` helps me find all text files within a given directory. And `sed \'s/oldtext/newtext/g\' file.txt` allows for quick text file editing. These are just snippets of the power the command line gives you.\\"\\n\\n## A quick tips\\n\\nIf you\'re just starting your command-line journey, don\'t feel overwhelmed. A great way to begin is by setting up a virtual machine, providing a safe sandbox to experiment without fear of damaging your system. Remember that the `man` command is your best friend, offering detailed documentation for any command you encounter. And don\'t hesitate to reach out for help online; the community is incredibly supportive and willing to guide you. Finally, personalize your terminal to make it visually appealing. A customized terminal can make the learning process more enjoyable and less intimidating. Start small, practice consistently, and you\'ll be amazed at how quickly you progress.\\n\\n## - Conclusion\\n\\nLearning the command line was one of the best investments I\'ve made in my tech skills. It\'s boosted my productivity, given me more control over my computer, and opened up a world of possibilities.\\n\\nSo, I encourage you to start your own journey. Open your terminal and type ls. See what happens! Find a basic online tutorial and follow along. You might be surprised at how quickly you pick it up.\\n\\nAnd remember, the command line isn\u2019t just for tech wizards. It\u2019s for anyone who wants to unlock the full potential of their computer."}]}}')}}]);